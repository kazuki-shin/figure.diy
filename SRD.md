# **Figure.DIY \-** Software Requirements Document (SRD)

Table of Contents

[1\. Introduction](#1.-introduction)

[2\. System Architecture](#2.-system-architecture)

[2.1 Hardware Architecture](#2.1-hardware-architecture)

[2.2 Software Stack](#2.2-software-stack)

[3\. Functional Requirements](#3.-functional-requirements)

[3.1 Voice Command Processing (Speech Interface)](#3.1-voice-command-processing-\(speech-interface\))

[3.2 Perception, Planning, and Control (Unified End-to-End Policy)](#3.2-perception,-planning,-and-control-\(unified-end-to-end-policy\))

[3.3 Robot Coordination (Dual-Arm Manipulation)](#3.3-robot-coordination-\(dual-arm-manipulation\))

[3.4 External Integrations & APIs](#3.4-external-integrations-&-apis)

[4\. Non-Functional Requirements](#4.-non-functional-requirements)

[4.1 Performance Constraints](#4.1-performance-constraints)

[4.2 System Robustness Considerations](#4.2-system-robustness-considerations)

[5\. Interfaces & Communication](#5.-interfaces-&-communication)

[5.1 Hardware-Software Interaction](#5.1-hardware-software-interaction)

[5.2 Network & Data Flow](#5.2-network-&-data-flow)

[6\. Deployment & Testing Strategy](#6.-deployment-&-testing-strategy)

[6.1 Simulation Testing (NVIDIA Isaac Lab & Virtual Environment)](#6.1-simulation-testing-\(nvidia-isaac-lab-&-virtual-environment\))

[6.2 Real-World Deployment (On Jetson Orin NX)](#6.2-real-world-deployment-\(on-jetson-orin-nx\))

[7\. Future Considerations & Enhancements](#7.-future-considerations-&-enhancements)

[7.1 Potential Hardware Improvements](#7.1-potential-hardware-improvements)

## 1\. Introduction {#1.-introduction}

The **Figure.DIY** project is a do-it-yourself replication of Figure AI’s impressive household chore demo, using affordable open-source robotics. In the original demo, two humanoid robots (Figure 02 models) responded to simple voice commands and **collaboratively put away groceries by scanning their environment and adapting in real time**. Our goal is to recreate this scenario with low-cost components, showcasing that everyday chores can be automated without proprietary hardware. This aligns closely with the hackathon’s theme of **democratizing AI robotics for daily life**, by making advanced home automation **accessible and reproducible** to makers and researchers.

**Scope:** This Software Requirements Document (SRD) covers the complete system, including both **software and hardware integration** aspects. It defines requirements for the AI software stack (speech recognition, vision processing, action planning, and control algorithms) as well as the hardware components (robotic arms, sensors, and computing unit) that together form the Figure.DIY system. The SRD will detail how software components interface with physical hardware, the functional behaviors expected (such as understanding voice commands and manipulating objects), and the non-functional constraints (like real-time performance and robustness given the DIY hardware). Both the **robotics control software and the hardware setup** are in scope, while low-level electronic design or detailed mechanical fabrication steps are out of scope (assumed to be given by the hardware kits). The document is structured into sections covering the system architecture, functional and non-functional requirements, interface definitions, constraints/assumptions, deployment strategy, and future enhancements.

## 2\. System Architecture {#2.-system-architecture}

### 2.1 Hardware Architecture {#2.1-hardware-architecture}

The hardware setup of Figure.DIY consists of the following key components:

- **Dual SO-ARM100 Robotic Arms:** The system uses two Standard Open ARM 100 (SO-ARM100) robotic arms mounted on a stable base or table. Each arm provides **6 degrees of freedom** (6-DOF) using high-precision servos, allowing a wide range of motion for tasks like reaching, grasping, and placing objects. The arms are constructed from 3D-printed parts to be lightweight and affordable. They utilize *Feetech STS3215 serial bus servos* with magnetic encoders for joint actuation. In the base **“AI Arm Kit” configuration**, each arm’s servos can provide up to \~19.5 kg·cm torque at 7.4V (or 30 kg·cm at 12V for the upgraded Pro kit). Each arm is accompanied by a dedicated motor control board (one per arm) that interfaces with the servos via a daisy-chained UART bus. The motor controllers connect to the main computer over USB (serial communication) to receive joint angle commands. Together, the dual arms enable bimanual manipulation – they can work in coordination for two-handed actions such as handing off objects or stabilizing one hand while the other operates, much like a human’s arms.  
    
- **reComputer J4012 (Jetson Orin NX 16GB):** This component is the **central computing unit** and “brain” of the robot. It is a compact edge AI computer built around the NVIDIA Jetson Orin NX system-on-module, providing up to 100 TOPS of AI performance for running deep learning models. The reComputer J4012 comes with an NVIDIA Ampere GPU (with 1024 CUDA cores and 32 Tensor cores on the Orin NX), a 6-core ARM CPU, 16 GB of RAM, and a 128 GB NVMe SSD preloaded with NVIDIA JetPack (Ubuntu-based Linux with AI libraries). **Rich I/O** is available for connecting peripherals – including 4× USB 3.2 ports (used for the arms’ USB control boards and sensors), an HDMI output, Ethernet, and CSI camera interfaces. This unit runs all the high-level software: the speech recognition, vision processing, policy inference, and servo control commands. Its GPU allows running neural networks (Whisper ASR, object detection, policy models) in real-time, while the CPU can handle coordination logic and sensor I/O. The Jetson board operates on 20W–40W of power, and the entire reComputer is **fan-cooled** in a small enclosure (approximately 13×12×6 cm) that can be mounted near the arms. This on-board compute enables **edge AI processing with low latency**, crucial for responsive control without relying on cloud services.

- **ReSpeaker Microphone Array:** Figure.DIY employs a **ReSpeaker** microphone array (from Seeed Studio) as the audio input device for voice commands. This is typically a USB-connected circular array of 4 digital microphones with built-in DSP (such as the ReSpeaker Mic Array v2.0). The array is capable of far-field voice capture (detecting speech up to 5 m away, even with background noise). It provides features like echo cancellation, noise suppression, and directional audio capture (DOA – Direction of Arrival detection) to better isolate the user’s voice commands. The microphone array connects via USB to the Jetson and appears as a standard audio input. By using a multi-microphone array, the system can reliably pick up the operator’s voice from across a room without a handheld microphone, enabling natural voice interaction with the robot. (Note: The ReSpeaker also has an onboard LED ring for visual feedback, which could be used to indicate listening status or to debug voice input, though that is not a core requirement.)  
    
- **reCamera (Vision Camera):** For visual perception, the Figure.DIY robot integrates a **reCamera**, which is a camera module used for AI vision applications. The reCamera 2002 series is an intelligent camera device that can stream HD video to the Jetson for processin. In this project, the camera acts as the robot’s “eyes,” mounted such that it covers the workspace where objects will be manipulated (e.g., overlooking a table or attached to one of the arms as an eye-in-hand configuration). The exact model can be a high-definition RGB camera (for example, a 1080p USB camera or the Seeed reCamera which includes an 8MP sensor and onboard chip for easy integration). The camera feed is used for real-time **object detection and scene understanding**. It connects via USB or CSI to the Jetson. The system may use multiple cameras (for instance, one static overhead camera and one camera on a robotic arm) to gain better perspective or depth, but at minimum one camera is required for core functionality.  
    
- **Other Hardware:** In addition to the main components, the setup includes standard supporting hardware: a power supply unit for the Jetson and servo motors (the arms require a stable DC supply, e.g., 7.4V or 12V depending on servo configuration), mounting fixtures (the arms can be clamped or bolted to a desk or frame for stability), and possibly a speaker (for audio feedback if needed – the ReSpeaker mic array includes an audio jack for output, but the SRD’s focus is on input). All components are **interconnected and coordinated** by the central Jetson computer, forming a complete robotic system capable of perception and action.

### 2.2 Software Stack {#2.2-software-stack}

The software architecture is built as a **stack of integrated AI and robotics frameworks** running on the Jetson Orin NX, orchestrating everything from sensor processing to low-level motor commands. The major software components include:

- **LeRobot Framework (Hugging Face’s Robotics Library):** At the core of the software stack is the **LeRobot** open-source framework. LeRobot is a PyTorch-based robotics library that provides models, datasets, and tools for real-world robot learning and control. It serves a role analogous to Hugging Face’s Transformers library, but for robotics – simplifying project setup by offering pretrained models and integration with simulators. In Figure.DIY, LeRobot is used as the high-level integration framework that ties together **imitation learning, reinforcement learning, and robot control** modules. It provides an API to define robot configurations (for the SO-ARM100 arms), load or train policies, and run inference to control the real robot. LeRobot comes with a suite of pre-trained models and example environments (e.g., *Libero*, *Aloha*, *Push-T* tasks) and supports data collection from teleoperation. For this project, it manages the overall flow: capturing sensor data (cameras, mic), feeding data into AI models (ASR, vision, policy), and outputting actions to the arms. Essentially, LeRobot acts as the **middleware and training framework**, enabling rapid development of the robot’s capabilities with modern ML techniques.  
    
- **OpenPI (Open Physical Intelligence) Action Policy:** The decision-making for the robot’s actions is powered by an **imitation learning policy** from the OpenPI project. OpenPI (by the Physical Intelligence team) provides large **vision-language-action models** – specifically the π0 (Pi-Zero) family of policies – which are generalist manipulation policies pre-trained on diverse robot data ([π0 and π0-FAST: Vision-Language-Action Models for General Robot Control](https://huggingface.co/blog/pi0#:~:text=manipulation%20tasks%20across%20different%20embodiments)). In Figure.DIY, an OpenPI policy model is employed to interpret high-level goals (derived from voice and vision inputs) and output low-level motor commands for the dual arms. The π0 model can generate smooth, real-time joint trajectories at 50 Hz, making it suitable for controlling the arms fluidly. We integrate OpenPI via LeRobot. The policy network likely takes as input an observation (which may include the camera image, object locations, and the parsed instruction) and produces an action vector (joint angle commands or end-effector motions) at each time step. The OpenPI model (possibly π0-FAST or a fine-tuned variant) runs on the Jetson’s GPU using PyTorch. Because training large policies on the Jetson is not feasible, any **policy training or fine-tuning is done offline** on a more powerful machine (no training compute limits, per project constraints) and the resulting model checkpoint is then deployed on the Jetson for inference. The OpenPI policy execution module thus forms the *“brains”* of task execution, enabling complex manipulation sequences learned from human demonstration data.  
    
- **OpenAI Whisper (Automatic Speech Recognition):** The **voice command processing** pipeline uses OpenAI’s Whisper model for automatic speech recognition (ASR). Whisper is a state-of-the-art transformer-based ASR model known for its high accuracy across many languages. In the Figure.DIY software stack, we utilize Whisper (likely a smaller variant of the model, such as Medium or Small, to fit real-time constraints on the Jetson) to convert spoken commands from the user into text. The Whisper model runs in PyTorch as well, taking audio input from the ReSpeaker mic and producing a text transcript of the command. We use Whisper in an offline mode (running fully on the Jetson) so that the system does not depend on an internet connection or external API for speech recognition. This choice leverages the Jetson’s AI acceleration to achieve fast inference. The output text from Whisper is then passed to downstream modules (for example, it could be parsed or fed into the OpenPI policy as part of the context for deciding actions, if the policy is conditioned on language). By using Whisper, the system can handle **natural language voice instructions** with robust accuracy, enabling a hands-free user interface.  
    
- **Computer Vision and Perception Modules:** To allow the robot to **see and recognize objects**, the software includes a vision component. This might involve a pre-trained object detection model (e.g., YOLOv5 or Detectron2) or an image classification model combined with localization. Running on the Jetson’s GPU, the vision model processes frames from the reCamera and identifies known objects or novel obstacles in the scene. For instance, a trained detector could recognize common household items (bottles, cups, tools) or at least detect graspable objects by bounding boxes. Additionally, we can integrate an **open-vocabulary model** (like CLIP or a segmentation model) to handle novel objects by description. The vision system will provide the **positions and identities of objects** to the planning module. This may involve using **depth estimation** if available (the reCamera or an added RealSense could give depth information) to locate objects in 3D space for the arms to grasp. The software likely uses OpenCV and PyTorch for these CV tasks. This module is also responsible for any **scene understanding** needed (e.g., detecting free space for placement, identifying if an object is held by an arm, etc.). The output of vision processing is formatted into a structured representation (for example, a list of detected objects with coordinates) that the action policy can use to make decisions.  
    
- **Motion Planning & Control:** Beneath the high-level policy, the system includes a **motion execution layer** responsible for translating planned actions into precise motor commands. This involves kinematics and coordination control for the two arms. The software uses the kinematic model of the SO-ARM100 arms (DH parameters or similar) to plan joint movements. For complex motions, a motion planner (potentially part of LeRobot or external like MoveIt if ROS was used) ensures that the path is collision-free (arms should not hit each other or the environment) and dynamically feasible given the servo capabilities. The control module interfaces directly with the arms’ servo controllers via a USB-to-UART connection. Commands are sent as target angles (or incremental movements) to each joint at a high frequency (e.g., 50 Hz or more, matching the policy output rate). Feedback from the servos (like current angle or error, if provided by the servo board) is used for rudimentary feedback control. This layer might implement **PID controllers or smoothing filters** to follow the desired trajectories accurately despite hardware limitations (to mitigate shakiness by not commanding abrupt moves). For bimanual tasks, the control module can synchronize movements of both arms, for example timing a handoff so that one arm releases an object exactly when the other has grasped it. This software component could be part of LeRobot’s runtime (since LeRobot likely supports sending commands to the specific hardware) or a custom interface using libraries provided by TheRobotStudio for the servos.  
    
- **Training and Data Processing Tools:** Since imitation learning is central to Figure.DIY, the software stack includes tooling for **data collection and model training** (though training itself is not done on the Jetson, the framework for it is included). For example, LeRobot provides scripts to record demonstrations (teleoperating the robot or using a “leader-follower” arrangement where a user manipulates one arm and the other copies that motion to collect data). Data (sequences of images, states, actions) can be logged and later used to train or fine-tune the policy in PyTorch. The project uses PyTorch as the primary machine learning library for model training tasks. Additionally, any necessary training pipeline components – such as augmentation of demonstrations, reward calculations for RL, or conversion of datasets to the required format – are part of the software design. This ensures that developers can improve the robot’s skills by gathering new data and retraining models iteratively. The trained models (Whisper, object detector, OpenPI policy, etc.) are saved and deployed on the Jetson for runtime use.  
    
- **Auxiliary Software:** Other software elements include an interface for **external APIs or cloud services** if needed (for instance, if occasionally offloading heavy vision processing to a remote server, though by design we aim for on-device inference). There may also be a voice response system or simple dialogue manager (e.g., to have the robot acknowledge commands or clarify instructions using text-to-speech, although this is not a primary requirement). Logging and monitoring tools are included so that all sensor inputs, decisions, and actions can be recorded for debugging or analysis. The system likely runs on an Ubuntu Linux OS (JetPack) and may use containerization (Docker) to encapsulate the environment given the complex dependencies (ROS, PyTorch, etc., could be managed in a container for ease of deployment). If ROS2 is used in some capacity (not mandatory since LeRobot can operate with PyTorch alone), then ROS topics would carry sensor data and command messages between nodes. In summary, the software stack is a combination of robotics middleware (LeRobot/possible ROS), AI models (Whisper ASR, CV model, OpenPI policy in PyTorch), and control interfaces, all optimized to run together on the Jetson Orin NX hardware.

The following diagram illustrates the high-level software architecture (from sensing to action) and how it ties to hardware components:

*(Diagram to be inserted: showing microphone \-\> Whisper ASR \-\> text; camera \-\> Vision model \-\> object data; text \+ object data \-\> OpenPI policy \-\> planned actions; actions \-\> motion controller \-\> arms. All running on Jetson.)*

## 3\. Functional Requirements {#3.-functional-requirements}

This section details the functional capabilities the Figure.DIY system must fulfill. These are organized by major feature: voice command processing, vision, action policy execution, dual-arm coordination, and external integrations.

### 3.1 Voice Command Processing (Speech Interface) {#3.1-voice-command-processing-(speech-interface)}

**Requirement:** The robot shall support **voice-controlled operation**. It must continuously or on-demand listen for user voice commands, accurately transcribe them to text, and interpret them to trigger the corresponding actions. This involves the following sub-requirements:

- The system uses the **Whisper ASR model** to convert spoken words into text in real time. It should handle natural-language instructions (e.g., “*Pick up the red cup from the table and hand it to me*”) and produce the full transcript with punctuation. The ASR component should achieve a high recognition accuracy for the user’s language (e.g., English) and be robust to various accents or moderate background noise (the microphone array’s noise reduction aids this).  
    
- The **latency of voice recognition** must be low enough to allow natural interaction. For instance, from the end of a user speaking a command to the system understanding it should be ideally under 2–3 seconds. This may require using a smaller Whisper model or partial decoding as speech is incoming. The Jetson Orin NX’s performance should be leveraged to run the model efficiently (possibly using FP16 or INT8 optimizations for Whisper via TensorRT, though implementation detail).  
    
- After obtaining the text of the command, the system needs to **parse and map the command to actions**. In many cases, the text command can be fed directly into the OpenPI policy model as part of its input (since π0 is a vision-language-action model that can take a language goal. If additional parsing is needed (for example, to extract an object name or target location from the sentence explicitly), a simple rule-based parser or NLP tool can be used. The functional requirement is that a spoken instruction relevant to the robot’s domain will result in the robot executing the correct task or responding with an error if it cannot.  
    
- The voice interface should handle **start/stop or mode switching commands** as well. For example, a voice command like “stop” or “abort” must immediately halt the robot’s motion (for safety). Commands like “go to sleep” might disable listening or put the robot in an idle state. Conversely, a wake word (like “Robot” or a specific cue) could be required before listening to tasks, to avoid accidental triggers. The SRD requirement is to define these interactions clearly: e.g., *The system shall implement a push-to-talk or wake-word mechanism to activate voice command processing in order to prevent inadvertent actions.*  
    
- **Feedback**: It’s desirable (though not strictly required) that the system provides feedback to the user after understanding a command. For example, it could reply “Okay, picking up the cup.” via a synthesized voice or an LED indicator. This assures the user that the command was heard correctly. For SRD purposes, the requirement can be stated as: *The system should confirm critical commands via audible or visual feedback.* (This is a softer requirement and may be part of the user interface considerations.)

In summary, the voice command processing must allow a user to control the robot naturally by speech, with the Whisper ASR ensuring accurate transcription of commands to be executed.

### 3.2 Perception, Planning, and Control (Unified End-to-End Policy) {#3.2-perception,-planning,-and-control-(unified-end-to-end-policy)}

Requirement: The robot shall implement a unified, end-to-end vision-language-action policy model (such as OpenPI π0), integrating perception, planning, and control into a single cohesive module. The following requirements elaborate this:

* **Unified Multimodal Input:** The policy model directly processes RGB images from the camera, proprioceptive feedback (joint angles, gripper state), and textual commands from the ASR system without separate, explicit detection or segmentation pipelines.

* **Implicit Object Recognition and Scene Understanding:** The model shall implicitly learn and recognize objects within the environment based on visual context and linguistic instructions, removing the necessity for pre-trained, explicit object detection models. The robot should accurately interpret natural language commands referencing objects (e.g., “Pick up the screwdriver” or “Put the red cup in the box") by directly associating them with visual information.

* **Real-time Processing and Control:** The policy inference shall operate at a minimum frequency of 10 Hz to support smooth and responsive robot motions. Model inference latency should remain consistently below 100 ms per frame to ensure fluid action execution. If necessary, intermediate actions will be interpolated to achieve continuous motion.

* **Action Generation and Execution:** The policy model shall directly output low-level control commands (joint positions, joint velocities, gripper open/close states) executable by the robotic arms without separate intermediate planning modules. Generated trajectories must inherently consider kinematic constraints, preventing joint limits, collisions between the arms, or unintended contact with environmental obstacles.

* **Error Handling and Robustness:** The system must detect execution failures (e.g., missed grasps, object drops) through proprioceptive feedback or visual context, autonomously attempting corrective actions. In the event of repeated failures (after two automatic retries), the system must notify the user clearly through visual (LED indicators) or audible alerts.

* **Safety and Collision Avoidance:** Collision avoidance (arm-to-arm and arm-to-environment) shall be implicitly managed by the policy model, leveraging learned constraints from training data. Safety mechanisms, including immediate responses to emergency stop commands (verbal "STOP\!" or a physical emergency stop), must be integrated to halt all robotic movements promptly within 0.5 seconds.

This unified end-to-end policy ensures seamless integration of perception, planning, and control, significantly simplifying the robot's architecture while maintaining robust performance and reliability.

### 3.3 Robot Coordination (Dual-Arm Manipulation) {#3.3-robot-coordination-(dual-arm-manipulation)}

**Requirement:** The Figure.DIY robot must coordinate its two arms to perform tasks that involve both manipulators, including passing objects between hands and placing objects with precision in constrained spaces. The control system needs to manage multi-arm motion planning and synchronization. Key details:

- **Coordinated Motion:** When both arms are engaged in a task, the system shall plan their trajectories jointly to avoid conflict. This means preventing physical **collision between the two arms** and ensuring they do not impede each other. For example, if both arms are reaching into a narrow space (like a shelf or box) from different sides, their motion must be choreographed so they can both fit without collision. The requirement can be stated as: *The system shall perform collision checking not just for arm vs. environment, but also arm-to-arm, to ensure safe simultaneous operation.* This likely involves real-time monitoring of each arm’s joint angles and positions and using known geometry to avoid intersection.  
    
- **Object Handoff:** The robot shall be capable of **handing off an object from one arm to the other**. This requires tight coordination: one arm holds the object, brings it to a transfer position, the other arm moves to that position, grasps the object, and the first arm releases. The system must synchronize the gripper actions and ensure the object remains stable during transfer. Functional requirement: *The control software must support multi-step coordinated actions such as object handoff, where timing between arms’ actions is managed (e.g., do not open the first gripper until the second has a firm hold).* This might be implemented by a special routine or by the learned policy (if the policy was trained on handoff demonstrations). Either way, the system should recognize a handoff scenario and execute it reliably.  
    
- **Constrained Space Placement:** The arms should be able to **place objects into tight or constrained locations** such as assembling something in a confined space or putting a item inside a container. This demands high precision in positioning. The requirement is: *The robot shall place objects within a tolerance of a few millimeters to the target location and orientation specified, as needed by the task.* Achieving this may involve slow, careful motions and possibly using sensory feedback (vision or even tactile if available). If a placement is delicate (like fitting a block into a slot), the system might employ a strategy of aligning and then gently inserting. From a software perspective, this might require a fine motion planning step where, once near the target, smaller incremental moves are made. The arms’ **encoders (12-bit) provide some precision** but might not be enough if the mechanics are shaky; so an approach could be to use vision to verify final placement (looking if the object is in the right spot).  
    
- **Grip and Lift Mechanics:** Coordination also implies proper grip control. The system must ensure that when a gripper closes on an object, it applies enough force (within servo capability) and the motion of lifting doesn’t exceed what the servo can hold. For example, lifting a heavier object too fast could cause the servo to stall or drop it. Thus, the requirement: *The robot shall modulate its lifting speed and path based on object weight (if known or inferred) and shall avoid sudden jerks to minimize strain on the servos.* In practice, since we may not know weight precisely, we assume objects are lightweight (due to hardware limits) and always move with moderate acceleration.  
    
- **Multi-Arm Task Sequencing:** For tasks where arms operate in sequence (not simultaneously), the system should coordinate the hand-off of control. For instance, consider placing a lid on a box: one arm holds the box, the other picks the lid and places it. The arms work towards a common goal but with different sub-tasks. The requirement: *The robot shall execute multi-step tasks involving both arms either in parallel or sequence, as dictated by the action policy or a task script.* This might involve a higher level planner that assigns roles to arms for a given task (e.g., arm1 holds, arm2 manipulates), or it might be embedded in the learned policy for that task.  
    
- **Calibration & Reference Frames:** Under coordination requirements is the need that both arms operate in a common coordinate frame. The system must calibrate the arms’ base positions relative to each other and the camera. The functional requirement: *The system shall maintain a unified world coordinate system so that an object detected by the camera can be reached by either arm and so that arms can meet at a common point for handoff.* This likely means the arms are fixed on a known rig and their base transformation is known. The calibration can be done once (during setup) by measuring distances or via a calibration routine (e.g., having arms touch a common point and adjusting frames). Without this, coordination and perception would fail, so it’s critical.

Overall, the robot control and coordination requirements ensure that having **two arms is an advantage** (allowing complex bimanual tasks) and not a hindrance (avoiding interference). The software should seamlessly handle one or both arms for a given task, orchestrating them as needed.

### 3.4 External Integrations & APIs {#3.4-external-integrations-&-apis}

**Requirement:** The Figure.DIY system shall provide well-defined interfaces for integrating external modules and for communication between the subsystems (vision, speech, control). This includes APIs for sensor data, control commands, and possibly third-party services. The integration points must be clearly documented to allow extension or replacement of components. Details include:

- **Vision API:** There should be a defined interface (function calls or ROS topics, etc.) through which the action policy or any part of the system can request the latest perception results. For example, a function get\_detected\_objects() might return a list of objects with their attributes, or if using ROS, a topic /perception/objects could continuously publish detection results. The requirement states that *the vision module’s output is accessible via a documented API, including the object ID/label, position, confidence score, and timestamp.* Similarly, if raw images are needed by other modules (maybe the policy wants the image), an interface for image frames should exist (like a shared memory or a subscriber to a camera topic).  
    
- **Speech interface:** After Whisper transcribes text, the text (and possibly a confidence or alternatives) should be provided to the rest of the system via a clear interface. Possibly a callback or message on\_command(text) triggers the policy module. The system might also incorporate a lightweight Natural Language Understanding (NLU) layer to map textual commands to a structured form (though the policy likely handles raw text). The requirement: *Provide an interface for the ASR output such that the command text is delivered to the planning module along with any contextual metadata (speaker ID, confidence).* This ensures modularity (one could swap Whisper for another ASR by adhering to the same interface).  
    
- **Robot Control API:** The low-level control of the arms should be abstracted behind a control API. For instance, a function or service move\_arm(arm\_id, joint\_angles) or set\_gripper(arm\_id, open/close) would encapsulate the serial communication to the servo board. If using LeRobot, this might already be provided (e.g., LeRobot might let you command end-effector poses and handle inverse kinematics internally). The SRD requires that *there be a clear separation between high-level planning and low-level actuation through an API or command protocol.* This also means if someone wanted to manually control the arm, they could use this API. Additionally, any sensor feedback from the arms (like current joint angle or servo status) should be accessible via the same interface.  
    
- **Inter-Module Communication:** The various software modules (ASR, Vision, Policy, Control) need to communicate. This could be within one process or across processes. The system might use a message bus like ROS topics or a simpler pub-sub mechanism. The requirement is: *Define the communication channels between modules, including message formats and update rates.* For example, the camera might publish images at 30Hz, the vision module publishes detections at 5Hz, the policy subscribes to both detections and ASR results, etc. If using ROS2, topics and services will be defined in the interface specification section of the documentation. If not using ROS, then e.g. Python function calls or socket messages would be described (but since LeRobot is PyTorch-based, likely these are in-process calls or a lightweight messaging). We ensure the documentation covers these interfaces so developers know how data flows.  
    
- **External Service APIs:** In case the system uses any external APIs (for instance, calling an online service for a more powerful model or using a cloud robotics API), these must be specified. One potential integration could be **cloud-based model updates**: for example, downloading the latest policy checkpoint from a repository (Hugging Face Hub or S3) when available. The requirement might state: *The system shall be capable of updating its AI models via an external API or repository access, given proper network connectivity.* This ensures longevity (the robot can improve over time). Another integration point could be home automation systems (if in future we want the robot to communicate with a smart home API to, say, confirm if the lights are off before going to another room – but that’s beyond current scope). For now, main external dependencies are the model sources (Whisper is an open model we bundle, OpenPI policy might be downloaded from a URL ([GitHub \- Physical-Intelligence/openpi](https://github.com/Physical-Intelligence/openpi#:~:text=By%20default%2C%20checkpoints%20are%20automatically,assets)), etc.) and possibly any voice synthesis API if used for responses.  
    
- **Open-Source Framework Hooks:** Because we are using open-source frameworks (LeRobot, etc.), the system should comply with their interfaces for datasets, training, etc. For example, LeRobot expects data in a certain format (as noted in their docs, converting data to LeRobot dataset format ([GitHub \- Physical-Intelligence/openpi](https://github.com/Physical-Intelligence/openpi#:~:text=1,policy%20server%20and%20running%20inference))). The requirement is that *the data collected from real robot or simulation is formatted and stored according to the LeRobot dataset specifications, so that the training pipeline can consume it directly.* This might involve writing conversion scripts (which are part of the integration details). Similarly, the policy model might be integrated as a PyTorch Module in LeRobot; we should ensure our usage aligns with their API (for instance, calling policy.infer() as in their examples ([GitHub \- Physical-Intelligence/openpi](https://github.com/Physical-Intelligence/openpi#:~:text=%7D%20action_chunk%20%3D%20policy.infer%28example%29%5B))).  
    
- **Documentation of Interfaces:** A crucial non-functional but related requirement is to maintain clear documentation of all APIs. This SRD will outline them at a high level (e.g., “Function X does Y, expects Z as input”). In implementation, we’d provide a developer guide with specifics. Ensuring that the *vision, speech, control, and policy modules each have defined input/output interfaces and that these are consistent* will make development and debugging easier.

In summary, external integrations and APIs are about making sure each part of the system talks to each other (and to any external tool) in a defined way, enabling modularity and future expansion. This prevents the architecture from becoming a monolithic black box; instead, each component can potentially be replaced or upgraded as long as it respects the interface.

## 4\. Non-Functional Requirements {#4.-non-functional-requirements}

Aside from the functional capabilities, Figure.DIY must meet several non-functional criteria, including performance targets and robustness given the use of open-source hardware. These requirements ensure the system operates efficiently, reliably, and safely.

### 4.1 Performance Constraints {#4.1-performance-constraints}

- **Real-Time Operation:** The system shall operate in (near) real-time for both perception and action. Specifically, voice commands should be processed with minimal lag, object detection should happen fast enough to not stall the task, and control loops should run at high frequency for smooth motion. On the Jetson Orin NX, the target is to maintain at least \~10 FPS for vision processing and \~50 Hz for control output as discussed. This real-time requirement means optimizing code (using GPU acceleration, possibly using C++ for critical loops or leveraging NVIDIA’s TensorRT for model inference). If any part of the pipeline threatens to be a bottleneck, it must be profiled and improved. The overall system latency from command to action start should ideally be under a few seconds, and once moving, the control jitter should be low (small variance in loop timing).  
    
- **Throughput for Training (No On-board Limitations):** While training models is out-of-scope for the runtime system, the project requirement explicitly states that we have *no compute limitations for training.* This means we assume access to powerful GPUs off-board for training or fine-tuning the models. Thus, the training pipeline can use large batches, heavy augmentations, etc., without worrying about Jetson limits. The resulting models, however, need to be deployed on Jetson. The implication is we might train a large model and then distill or compress it to run on edge. Non-functionally, this requires a strategy for model compression if needed (like knowledge distillation, quantization). However, the SRD mainly notes that training can be done elsewhere, so model performance on Jetson is the main concern at runtime.  
    
- **Resource Utilization:** The system should efficiently utilize the **16GB memory** and GPU compute of the Jetson without exceeding it. For instance, running Whisper, a vision model, and a policy concurrently can be heavy; memory management (like loading models on demand or using half precision) might be needed. The requirement: *The software shall run within the resource limits of the Jetson NX (16 GB RAM, 8 CPU cores, GPU) without crashes or out-of-memory issues, and ideally leaving some headroom (e.g., \<= 80% utilization) to ensure stability.* The power envelope (Orin NX can throttle if overheated) should also be considered – maybe ensure models are efficient enough that the Jetson can run them sustained without thermal throttling in typical ambient conditions.  
    
- **Accuracy and Reliability:** Performance is also measured in how well it achieves tasks. For ASR, Whisper has known accuracy benchmarks – we aim for near state-of-the-art transcription accuracy on typical commands. For object detection, we aim for high precision/recall on relevant objects. For task success, an acceptable criterion might be: e.g., “the robot succeeds in executing a simple pick-and-place command 9 out of 10 times under nominal conditions.” These quantitative targets ensure the system is not only fast but effective. We might list some: *ASR word error rate \< 5% for clear commands; Vision detection confidence \> 0.8 for objects in clear view; Policy success rate \> 90% on trained tasks.* These are not hard requirements but goals to measure against.  
    
- **Jetson-Specific Optimizations:** The software should leverage NVIDIA Jetson optimizations: using CUDA for vision (OpenCV CUDA or custom kernels), using Tensor Cores for neural nets, etc. We should design with that in mind. Also, concurrency – e.g., using multiple CPU threads for I/O and preprocessing while GPU does inference – will improve throughput. The system should be **responsive**, meaning it can handle the asynchronous nature of input (voice might come while it’s doing something – the system should possibly be able to queue a new command or interrupt safely). If multi-command sequences are expected, maybe one at a time is enforced to keep things simple.  
    
- **No Hard Real-Time Guarantee**: Given this is a prototype/DIY project, we do not require formal hard real-time guarantees (like an RTOS would provide). Linux on Jetson should be sufficient, though we might set thread priorities for the control loop. The requirement is that perceived performance by the user and the task is adequate (no noticeable stutter or dangerous lag in movement).

### 4.2 System Robustness Considerations {#4.2-system-robustness-considerations}

- **Mechanical Stability and Error Tolerance:** The open-source hardware nature means the arms may not have the rigidity or precision of industrial robots. There could be slight flex in the 3D printed parts and backlash in the servo gears, causing “shakiness” or oscillations, especially when moving fast or carrying near-max weight. The system must be robust to these imperfections. For instance, controllers might include a damping factor to avoid overshooting and oscillating. The requirement: *The control algorithms shall account for the arm’s flexibility by using slower acceleration profiles and possibly feedback adjustments to minimize oscillations.* Essentially, we trade speed for stability. If the arm wobbles after a move, the system might wait a short moment for it to settle before the next precise action (like before releasing an object).  
    
- **Weight and Force Limits:** The servos (19 kg·cm torque) limit the payload—likely only a few hundred grams can be lifted at full reach. Trying to lift heavier or forcing the arm can cause servo stalls or even strip gears. So the robot should not attempt tasks beyond its capacity. The system can have a configured maximum payload and refuse or warn if asked to do something requiring more (for example, if it detects an object that’s likely too heavy—maybe by size, or simply a configured list). *The system shall enforce operational limits (joint torque, speed) to avoid damaging the servos.* This might be implemented as current sensing if available, or as conservative estimates in software.  
    
- **Servo Weakness and Thermal:** Prolonged use or holding a weight can heat up servos or drain them. The system should ideally monitor servo status. Some smart servos provide feedback like temperature or load; if the STS3215 does, we can use that. If a servo becomes unresponsive or overheated, the system should pause and alert. *The robot shall include a safety check that stops motion if a servo feedback indicates a fault (e.g., no position change despite command or temperature beyond threshold).* This prevents burning out motors.  
    
- **Electrical and Communication Resilience:** The UART bus controlling the servos is susceptible to communication errors (especially as multiple servos chain). The software should handle communication timeouts or retries gracefully. For instance, if a command to move joint 3 fails to send or times out, try again or stop if it persists, rather than assuming it moved. Also, ensure the power supply is stable; if voltage drops when motors draw current, it could reset the controller. A hardware fix (sufficient PSU and maybe capacitors) is assumed, but software should be ready for a controller reconnect. *Requirement:* log and attempt to recover from communication errors with motor controllers without crashing the whole system.  
    
- **Safety Mechanisms:** Robustness includes safety for users. The arms should not move in a way that could harm a person. Although they are relatively small, a servo at full speed could pinch or hit. The system should have an easy **Emergency Stop** mechanism – possibly a voice command “stop” or a physical kill switch – that instantly cuts power to motors or sends a stop command. This is crucial; we define that: *The system shall implement an emergency stop that brings the robot to a halt within \<0.5 seconds in any state.* In practice, cutting power might let arms collapse due to gravity, so better is to command a stop motion (servo hold position) via software and then gently disable. Regardless, safety is a non-functional requirement not to be overlooked.  
    
- **Robustness of Open-Source Software:** The project uses many open-source libraries. We need to ensure they are configured correctly to avoid crashes. For example, running on the latest JetPack, some PyTorch or CUDA versions might have bugs. We should use stable versions and test thoroughly. The requirement might mention: *The software environment (dependencies) shall be managed and versions locked to known stable releases to prevent unpredictable behavior due to upstream changes.* Additionally, since LeRobot is relatively new, if issues are encountered (memory leaks, etc.), our system should be designed to handle or mitigate them (like periodic resets or using a separate process for any unstable component so it can be restarted without killing the main system).  
    
- **Reset and Recovery:** The system should be able to recover from certain faults. For instance, if the arms lose calibration (maybe a servo skips steps or gets misaligned due to an impact), the system should be re-calibratable without a full rebuild. A calibration procedure (moving to known home positions and resetting encoders) should be available. Non-functionally, this means documenting a process for calibration and possibly having the software automate part of it (like slowly move to a mechanical stop or marker to recalibrate).

In short, robustness for Figure.DIY means the system will not easily break or fail catastrophically due to the inherent weaknesses of DIY hardware. It should handle minor issues gracefully and avoid pushing the hardware beyond safe limits. Testing under various conditions (long durations, different load) will be done to validate these aspects.

## 5\. Interfaces & Communication {#5.-interfaces-&-communication}

This section describes how different parts of the system communicate, including hardware-to-software interfaces and internal data flows. It covers protocols, data formats, and any network considerations.

### 5.1 Hardware-Software Interaction {#5.1-hardware-software-interaction}

- **Robotic Arms Interface:** The two SO-ARM100 arms interface with the Jetson through their motor control boards. Each board likely connects via USB and is presented to the OS as a serial port (e.g., /dev/ttyACM0). The communication protocol to control the STS3215 servos is a UART serial bus protocol (possibly half-duplex at a certain baud rate). The software will send commands to set servo positions and read servo states using this serial link ([Playing with Serial Bus Servo Motors](https://adityakamath.github.io/2022-05-08-akros-final-update/#:~:text=Image%203%20Feetech%20STS3215%20motors,4V%2C%20which%20I%27m)). The system uses a **driver or library** (possibly provided by TheRobotStudio or through LeRobot) to format these commands. Typically, one needs to set unique IDs for each servo on the bus (e.g., 6 servos each arm have IDs 1-6 on arm1’s bus, 1-6 on arm2’s bus, differentiable by port). The interface ensures we can address each joint individually. For example, a command packet might include servo ID and target angle. The interface can be abstracted such that the control module can call a function like set\_joint\_angle(arm\_id, joint\_index, angle) and the driver will send the appropriate bytes over USB. The servo feedback (like current angle or success acknowledgment) can be read similarly. Timing-wise, we might send updates at 50Hz; the serial bandwidth must support that for 12 servos – which it should, as commands are just a few bytes each at say 115200 baud or higher.  
    
- **Camera Interface:** The reCamera (or any USB camera) interfaces through either a V4L2 driver (if USB webcam) or a specific API (if it’s the smart reCamera, it might enumerate as a network device or USB storage type interface; but likely simpler to treat it as a standard UVC camera). The software uses OpenCV or GStreamer to grab frames from the camera at a set resolution (e.g., 1280×720) and frame rate. If using multiple cameras, each will have a device path (/dev/video0, /dev/video1 etc.). The camera provides raw image frames (likely in YUV or BGR format) which the vision module processes. If reCamera has its own processing, we might not use that feature, instead directly streaming to Jetson for full control. The interface to the vision system is basically the frames in memory; if using ROS, that would be a sensor\_msgs/Image topic, or if not, just an OpenCV Mat passed to the detector.  
    
- **Microphone Interface:** The ReSpeaker mic array connects via USB and is accessed as an audio input device (likely through ALSA or PulseAudio on the system). The Whisper ASR will need audio data, typically a stream of 16 kHz PCM. The interface here could be implemented by recording audio via a library like PyAudio or sounddevice in Python, or using the OS’s voice capture. The system should manage the microphone input either continuously or in segments triggered by a wake-word (which might need a simple always-on keyword detector, possibly using the mic array’s DSP if it has one). For simplicity, we can run Whisper in a loop that listens for a phrase (maybe using voice activity detection to know when to start/stop listening). The key interface detail: audio buffers are fed into the Whisper model (which expects a certain format, e.g., 30-second chunks of 16kHz audio for full context, but can do shorter). We may also downsample or convert stereo to mono as needed. The microphone likely captures at e.g. 48kHz, so we resample to 16kHz. All this is internal detail; externally, the main interface is that *the microphone provides an audio stream to the ASR component.* No complex protocol, just PCM frames. If needed, the ReSpeaker’s beamforming could be configured (some arrays allow selecting a beam direction or turning on/off certain processing via USB HID commands) – we might not expose that at SRD level, just assume default mode works.  
    
- **Compute to Hardware Timing:** The communication with hardware should be scheduled properly. For example, sending many servo commands too fast could saturate the serial link. So there might be a need for slight delays or combining commands if the protocol allows (some servo protocols let you send a batch of angle commands to multiple IDs in one packet). The interface design could choose to send a sync command that moves all joints simultaneously rather than sequentially. If available, that’s preferred to ensure coordinated motion (i.e., the servos move together rather than one after the other). This detail may be part of the servo API.  
    
- **State Feedback:** Interface is bi-directional – the robot can query joint angles from the servo board, or the servo might automatically return the final angle after a move. The software should use this to update its internal notion of the arm’s pose (for instance, to update the simulation or for the policy observation if needed). The communication interface should support read requests: e.g., get\_joint\_angle(arm\_id, joint\_index) returns the last known or queries fresh from the servo. If performance is an issue, we might trust our commanded angle as the state (since these servos likely use closed-loop control to reach the commanded position), but small error might exist.  
    
- **Hardware Sync:** The arms, camera, and microphone are largely independent hardware. But for certain tasks, time synchronization might matter (less so here, since we can treat them sequentially: audio processing doesn’t need to be time-aligned with video, except perhaps if the command references something currently happening in video – not likely). However, if we had, say, a scenario where a command “now\!” corresponds to a current visual state, a sync could be needed. That’s beyond current scope, likely fine to assume sequential processing.

In summary, hardware-software interaction is managed through USB interfaces (serial for arms, audio for mic, UVC for camera), using standard protocols or libraries. The SRD ensures these interfaces are understood so the implementation can proceed with the correct drivers and data handling.

### 5.2 Network & Data Flow {#5.2-network-&-data-flow}

- **Data Flow Overview:** The internal data flow can be summarized as:  
    
  - **Audio data** flows from microphone \-\> ASR \-\> text.  
  - **Image data** flows from camera \-\> vision model \-\> object info.  
  - **State data** (like current arm positions) flows from servo feedback or internal tracker \-\> planning module.  
  - These all converge in the **action policy module**, which produces action commands.  
  - Then **action commands** flow to the motor controllers to move the arms.  
  - Throughout this, there may be logging data flowing to storage (for training logs or debug).


  The system architecture can be considered a pipeline with parallel sensory streams feeding into a decision node, then an actuation stream going out to hardware.


- **Model Update Flow:** If the robot is improved over time, new models might be downloaded or loaded from storage. For example, after offline training, a new policy checkpoint is placed on the Jetson (via SD card, USB, or network). The software should be able to load it on startup or on command. Possibly an update service could run to fetch the latest from a repository (as an *external integration* mentioned). In terms of data flow, this is typically manual or through a script; we likely won’t have the robot autonomously retrain itself (except maybe collecting data for later training). But we ensure that model files are read from a configurable location, and the system can be restarted or signaled to use a new model without major code changes.  
    
- **Telemetry and Logging:** For both debugging and future learning, the system will log data (sensor readings, actions taken, maybe video). If a network connection is available, it might stream telemetry to a remote dashboard or simply save locally. The data flow from sensors to log could be high volume (video), so likely only key events or low-rate state is logged by default, with option to record full data when needed (like a “record mode” for collecting demonstration – which LeRobot has ([Building a 3D-Printable robot arm with LeRobot : r/3Dprinting](https://www.reddit.com/r/3Dprinting/comments/1gs1lj8/building_a_3dprintable_robot_arm_with_lerobot/#:~:text=What%20I%20Did%3A))). The SRD could specify: *The system shall provide a mechanism to record trajectories (sequence of images, states, actions) either to local storage or over network, for debugging or training purposes.* This implies a data flow from runtime to disk or to another computer.  
    
- **Network Interfaces:** By default, Figure.DIY is an edge device that can operate offline. However, network could be used for:  
    
  - SSH/remote control by a developer.  
  - Streaming a video feed to a remote UI (maybe the user wants to telemonitor the robot).  
  - Offloading computation: as hinted, one could run the heavy policy on a server and send actions via WebSocket ([GitHub \- Physical-Intelligence/openpi](https://github.com/Physical-Intelligence/openpi#:~:text=Remote%20Inference%3A%20We%20provide%20examples,robot%20and%20policy%20environments%20separate)). If we consider that, the data flow would be: Jetson sends current observation (images, states) to server \-\> server computes action \-\> server sends back action \-\> Jetson applies it. This adds latency but could allow using a bigger model. The requirement said "no compute limit for training" but not explicitly for inference, but left open. We can allow this as an extension: *The system may support an optional remote inference mode where perception is done on-board, but high-level action planning is done on a remote server, with communication via a network socket. In such a case, network latency must be low (e.g., within 50ms one-way) to preserve control smoothness.* This is an advanced scenario; by default, we assume on-board inference.


- **Latency Considerations:** Real-time control demands low latency. Within the Jetson, data flows over memory (fast). The critical path is from camera \-\> detection \-\> policy \-\> servo. That should be optimized. If network is involved (like remote inference), that path adds network latency. The SRD likely assumes local processing to meet the requirement of real-time. So network latency is not a big factor unless remote monitoring or such. But we should mention that the **system is largely self-contained**, requiring network only for optional tasks (like model update or developer access). This means even if the network is down, the robot can perform tasks from locally stored models.  
    
- **Concurrency and Synchronization:** The data flows might be in parallel threads or processes. We must ensure thread-safe communication. For example, vision might be running continuously, updating a global object list. The policy might take a snapshot of that when deciding. We either lock or use message passing to avoid race conditions. This is a design detail but should be noted: *the system’s internal communication is designed to handle asynchronous data (e.g., a new voice command can interrupt the current action if needed, or at least be queued).*  
    
- **Bandwidth:** Internal data flows (camera feed at maybe a few MB/s, audio negligible, commands negligible) are within Jetson memory – fine. If remote telemetry is used, streaming video could use a lot of bandwidth, but that's optional. If multiple cameras at high res, the CSI or USB bandwidth might need consideration – but one 1080p 30fps is fine on USB3. The microphone is minor (audio is small). Serial to servos also small. So no major bandwidth issues on internal buses as long as using USB3 for camera.  
    
- **Error Handling in Communication:** If any data flow is disrupted (camera disconnects, mic error), the system should detect and handle it. E.g., if the camera feed goes black, maybe pause tasks and alert user. If mic stops, maybe default to not receiving commands until fixed. The architecture should allow modules to report errors up. Possibly using exceptions or status codes. Non-functional, but it ties to robust communication.

The data flow design ensures that all parts of the system get the information they need in a timely manner and that the system can optionally interface with external networks for extended functionality.

## 6\. Deployment & Testing Strategy {#6.-deployment-&-testing-strategy}

This section outlines how the system will be deployed on the actual hardware and the strategy for testing its functionality safely, including simulation phases and real-world trials. It also covers fail-safe mechanisms to handle issues during operation.

### 6.1 Simulation Testing (NVIDIA Isaac Lab & Virtual Environment) {#6.1-simulation-testing-(nvidia-isaac-lab-&-virtual-environment)}

Before running the system on the real hardware, we will validate it in a simulated environment. The simulation platform of choice is **NVIDIA Isaac Sim/Isaac Gym**, which allows high-fidelity physics simulation of robots on the Jetson platform or a PC. The steps are:

- **Virtual Model of Robot:** We create or obtain a URDF (Unified Robot Description Format) model of the SO-ARM100 arm. Given it’s open source, a URDF might already be available in TheRobotStudio repository or LeRobot examples. This model includes the arm kinematics, joint limits, etc. If two arms are used, we set up two instances in the sim, attached to a virtual table. We also simulate the camera (virtual camera sensor in Isaac) and possibly the microphone (though for voice, we might skip simulation – we can directly feed text commands in tests).  
    
- **Simulated Tasks:** In Isaac Sim, we can set up sample scenes replicating a household environment or just a table with objects (cubes, cylinders to represent cups, etc.). The testing strategy is to run through the core scenarios: e.g., “pick object and place in box” fully in sim. The perception in sim can be aided by perfect info (since we *know* object positions in sim, we could bypass the detection for initial tests to isolate the policy). Then gradually, we test the vision by rendering scenes and running our detection on those images.  
    
- **Integration Testing:** We run the entire software stack with simulation in the loop (rather than the real hardware drivers). If using ROS or Isaac’s messaging, we connect the policy to simulated arm joints. This will verify the action policy logic, multi-arm coordination, and ensure no major software bugs (like data not flowing, or some module crashing) in a safe setting. We especially want to test boundary cases: does the policy try to move beyond joint limits? Does the voice command parser properly handle various phrasing? Simulation allows quick iteration – if something goes wrong, we tweak code and re-run, without risking hardware.  
    
- **Performance in Sim:** We can also gauge if our loop can run at required speeds by observing sim. If sim at 1x real-time with our code struggles, real might too. Possibly we test on a PC with a powerful GPU for faster-than-real-time sim, but ultimately run a slower real-time sim on Jetson to see if it holds up.  
    
- **Isaac’s tools:** Isaac provides tools for domain randomization (to test vision robustness with different lighting, object colors) – we could use that to ensure our detector works in varied conditions before deployment. Also, we might use Isaac’s ability to simulate noise in actuators to see how control deals with slight errors.  
    
- **Milestone:** Only when the robot consistently completes tasks in simulation (with perhaps small tolerances, like maybe it “almost” picks an object but due to sim friction it slips – we accept that if logically it did right thing), we move to real hardware. This prevents simple logic mistakes or major failures from happening on the physical robot.

### 6.2 Real-World Deployment (On Jetson Orin NX) {#6.2-real-world-deployment-(on-jetson-orin-nx)}

Deploying to the actual robot involves setting up the Jetson with all necessary software and then incrementally testing on hardware:

- **Software Deployment:** The Jetson Orin NX will be flashed with NVIDIA JetPack (Ubuntu 20.04 with ROS2, CUDA, etc.). We then install our software stack: PyTorch (likely a Jetson-optimized build), Whisper model weights, LeRobot library, any drivers for ReSpeaker (the mic might need a firmware or driver installation), and so on. This environment setup can be encapsulated in a Docker container or done manually with documentation. The SRD ensures we plan this so that another developer can replicate the environment.  
    
- **Calibration and Initialization:** Before autonomous runs, we perform calibration. That involves:  
    
  - Ensuring each servo is identified and responds. We might run a calibration script from LeRobot to zero the arms (moving them to a reference position and recording offsets) ([How to use the SO100Arm robotic arm in Lerobot | Seeed Studio Wiki](https://wiki.seeedstudio.com/lerobot_so100m/#:~:text=caution)).  
  - Calibrating the camera extrinsics: find the transform from camera to arm base. This could be done by a one-time procedure like having the arm hold a distinctive marker in view and solve for transform.  
  - If the two arms need calibration relative to each other, do that now (though if both are fixed to the same frame by construction, measuring and setting their base transform is sufficient).


- **Unit Testing on Hardware:** Start with basic tests:  
    
  - Move each joint individually via a simple script to ensure servo control works (no load).  
  - Test gripper open/close.  
  - Test reading microphone input (maybe print transcribed text of a test phrase).  
  - Test capturing an image from the camera and running the detection model offline (like point camera at an object and see if it's detected). These ensure sensors and actuators are working with our code.


- **Incremental Task Testing:** Choose a simple task that involves minimal risk. For example, place one object within easy reach, and issue a voice command to pick it up and place it somewhere else on the same table. Observe the robot's behavior closely. Initially, be ready to use an emergency stop if it goes awry (like if an arm might collide or knock something over unexpectedly). The first runs might reveal calibration offsets (maybe it misses the object by 2cm – then we adjust or compensate in code). We refine until it can reliably do the simple task.  
    
- **Expanding Tests:** Introduce more complexity gradually:  
    
  - Different objects (to test vision generalization).  
  - Two objects (ensure it picks the right one, or if asked to move two sequentially, it can do it).  
  - Use both arms in a task (maybe have each pick up different objects simultaneously if policy allows, or a handoff scenario).  
  - Constrained space test: perhaps put a box that the robot must place something into; watch for collisions.  
  - Edge cases: no object present (should gracefully say “can’t find it”), voice misheard (maybe Whisper will rarely produce gibberish; ensure that doesn’t trigger a crazy action – a safety might be if command not understood, do nothing or ask again).


- **Performance Tuning:** While testing, measure how long things take. If voice command processing is too slow, consider adjustments (maybe use a smaller model, or run partially). If motions are too jerky, slow down speed parameters. This is the phase to fine-tune parameters in the software to match reality (e.g., friction might require a slightly stronger grip than in sim).  
    
- **Safety Observations:** Confirm that the robot’s emergency stop works. Intentionally issue one mid-task to see that it halts. Also check that if something like an obstacle is introduced unexpectedly (like a person’s hand), the robot either stops (if we have implemented any collision detection via current/feedback) or at least moves slowly enough not to harm (we may include a rule: if a new object appears in camera very close to robot’s path, stop – though that might not be real-time enough for fast reaction; we rely mostly on slow speed for safety).  
    
- **Documentation of Results:** As each test passes, we document that the requirement is met. For example, note that voice command X was executed successfully, object detection recognized Y correctly, etc. If some tasks fail consistently, that indicates either a requirement we set is not realistic or an implementation bug to fix.

## 7\. Future Considerations & Enhancements {#7.-future-considerations-&-enhancements}

Looking beyond the initial implementation, there are several areas where Figure.DIY could be improved or extended. These future considerations address hardware upgrades, software enhancements, and new capabilities that could be added to increase the robot’s performance and utility.

### 7.1 Potential Hardware Improvements {#7.1-potential-hardware-improvements}

- **Stronger & More Stable Actuators:** One obvious upgrade is to improve the **servo motors or arm design** to address the shakiness and strength limitations. For example, replacing the STS3215 servos with higher torque, metal-geared servos (like the 30kg·cm ones or even switching to Dynamixel smart servos) would allow the robot to handle heavier objects and move more confidently. Some community efforts already explore servo upgrades ([Servo Upgrade\! for the SO-ARM-100 \#robot \#lowcost ... \- YouTube](https://www.youtube.com/watch?v=b5HZOQ87Nsg#:~:text=Servo%20Upgrade%21%20for%20the%20SO,ARM100%2C)). Additionally, adding **braces or using better materials** for key structural parts of the arms could reduce flex. Future versions of the DIY arm might incorporate aluminum or carbon fiber components while keeping cost low.  
    
- **End Effector Enhancements:** The current gripper is basic. In the future, one could attach different end effectors: for instance, a suction cup for picking up flat objects, or a more dexterous two-finger gripper with touch sensors. This would expand the range of objects that can be manipulated (delicate objects, very small items, etc.). If a quick-change mechanism is possible, the robot could even swap tools mid-task (though that adds complexity).  
    
- **Sensor Upgrades:** For vision, adding a depth camera (stereo or LiDAR) would greatly help in perceiving 3D structure of the environment, making placement in constrained spaces easier by providing precise depth information. The reCamera could potentially be replaced or augmented by an Intel RealSense or an Azure Kinect in the future. For the microphone, if we find limitations (like trouble with noise), we might add a dedicated speech recognition IC or an array with more mics to improve beamforming. Additionally, including **tactile sensors** or force sensors on the gripper would give the policy feedback on whether it has successfully grabbed something and with how much force, improving reliability in grasping.  
    
- **Mobile Base:** Currently, Figure.DIY is a stationary unit. A major enhancement could be mounting the whole setup on a mobile base (like a wheeled robot or a small rover). That would transform it into a mobile manipulator that can navigate a home. However, this introduces SLAM (simultaneous localization and mapping) and navigation challenges and would significantly increase complexity. It’s a future direction for making the system more practical in large environments (e.g., it could drive to the kitchen to fetch an item). For now, we assume static, but we note the architecture could be extended with a ROS navigation stack if mobility were added.  
    
- **Better Compute or Cloud Offloading:** While the Jetson Orin NX is powerful, future AI models (or running multiple heavy models at once) might demand more. Upgrading to a Jetson AGX Orin (which has more GPU cores, up to 200 TOPS) could be considered if needed for advanced features. Alternatively, integrating a mechanism for **cloud offloading** in non-real-time parts (like using a cloud service to do a heavy planning computation in the background or fetch a solution to a new task) could be explored. Ensuring security and reliability in such offloading would be part of that enhancement.  
    
- **Battery Power Option:** To make the robot more portable (especially if a mobile base is added), a battery power system could be integrated. This would involve power management hardware and ensuring the Jetson and servos get stable power from battery. It’s doable but would need careful design to provide enough current for servos and enough voltage regulation for Jetson. A future version might have a swappable battery pack enabling about an hour of untethered operation.

In conclusion, Figure.DIY is designed with a modular architecture that can be iteratively improved. Future enhancements aim to make the robot **more capable, more robust, and more user-friendly**. By incrementally upgrading hardware and refining software (guided by user feedback and technological advances), Figure.DIY could evolve from a basic prototype to a truly helpful home robot platform. The SRD provides a foundation that anticipates these possibilities, ensuring the system built today won’t hit a dead-end and can adapt to tomorrow’s innovations.  